{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect Three \n",
    "\n",
    "The primary description of this coursework is available on the CM20252 Moodle page. This is the Jupyter notebook you must complete and submit to receive marks. This notebook adds additional detail to the coursework specification but does not repeat the information that has already been provided there. \n",
    "\n",
    "You must follow all instructions given in this notebook precisely.\n",
    "\n",
    "Restart the kernel and run all cells before submitting the notebook. This will guarantee that we will be able to run your code for testing. Remember to save your work regularly.\n",
    "\n",
    "__You will develop players for Connect-Three on a grid that is 5 columns wide and 3 rows high. An example is shown below showing a win for Player Red.__\n",
    "\n",
    "<img src=\"images/connect3.png\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "For your reference, below is a visual depiction of the agent-environment interface in reinforcment learning. The interaction of the agent with its environments starts at decision stage $t=0$ with the observation of the current state $s_0$. (Notice that there is no reward at this initial stage.) The agent then chooses an action to execute at decision stage $t=1$. The environment responds by changing its state to $s_1$ and returning the numerical reward signal $r_1$. \n",
    "\n",
    "<img src=\"images/agent-environment.png\" style=\"width: 500px;\"/>\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "Below, we provide some code that will be useful for implementing parts of this interface. You are not obligated to use this code; please feel free to develop your own code from scratch. \n",
    "\n",
    "### Code details\n",
    "\n",
    "We provide a `Connect` class that you can use to simulate Connect-Three games. The following cells in this section will walk you through the basic usage of this class by playing a couple of games.\n",
    "\n",
    "We import the `connect` module and create a Connect-Three environmnet called `env`. The constructor method has one argument called `verbose`. If `verbose=True`, the `Connect` object will regularly print the progress of the game. This is useful for getting to know the provided code, debugging your code, or if you just want to play around. You will want to set `verbose=False` when you run hundreds of episodes to complete the marked exercises.\n",
    "\n",
    "This `Connect` environment uses the strings `'o'` and `'x'` instead of different disk colors in order to distuingish between the two players.\n",
    "\n",
    "Before we start a game, we should call the `reset()` method. This method cleans the board and resets other state variables. The `first_player` argument can be specified (`'o'` or `'x'`) to deterministically choose the starting player. It defaults to `\"random\"`, in which case each player starts the game with probability of $\\frac{1}{2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game has been reset.\n",
      "[[' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ']]\n"
     ]
    }
   ],
   "source": [
    "import connect\n",
    "env = connect.Connect(verbose=True)\n",
    "env.reset(first_player='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can interact with the environment using the `act()` method. This method takes an `action` as input and computes the response of the environment. An action is defined as the column index that a disk is dropped into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' 'o' ' ' ' ']]\n"
     ]
    }
   ],
   "source": [
    "env.act(action=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `act()` method inserts a disk into the specified column. \n",
    "\n",
    "If we want to change the player on move, we can do so by using the `change_turn()` method. We can check whose turn it is by accessing the `.player_at_turn` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current player at turn: o\n",
      "Current player at turn: x\n",
      "[[' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' 'x' ' ' ' ']\n",
      " [' ' ' ' 'o' ' ' ' ']]\n"
     ]
    }
   ],
   "source": [
    "print(\"Current player at turn:\", env.player_at_turn)\n",
    "env.change_turn()\n",
    "print(\"Current player at turn:\", env.player_at_turn)\n",
    "\n",
    "# Drop another disk into the same centre column.\n",
    "env.act(action=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we set `verbose=True`, the grid is printed each time we call the `act()` method. This grid is stored as a two-dimensional numpy array in the connect class and you can easily access by calling..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' 'o' ' ' ' ']\n",
      " [' ' ' ' 'x' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ']]\n"
     ]
    }
   ],
   "source": [
    "current_grid = env.grid\n",
    "print(current_grid)\n",
    "# Notice that the grid now appears to be \"upside down\" because numpy arrays are printed from \"top to bottom\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' 'o' ' ' ' ']\n",
      " [' ' ' ' 'x' ' ' ' ']\n",
      " [' ' ' ' 'o' ' ' ' ']]\n"
     ]
    }
   ],
   "source": [
    "# Let's make another move.\n",
    "env.change_turn()\n",
    "env.act(action=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we make another move with `act(action=2)`, the environment will throw an error because that column is already filled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for axis 0 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-3460a887c4c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# This cell should throw an error!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchange_turn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\GitHub\\connect_three\\connect.py\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0man\u001b[0m \u001b[0minteger\u001b[0m \u001b[0mreferring\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0mindex\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnew\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mdisk\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mdropped\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \"\"\"\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlowest_free_rows\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplayer_at_turn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlowest_free_rows\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlowest_free_rows\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 3 is out of bounds for axis 0 with size 3"
     ]
    }
   ],
   "source": [
    "# This cell should throw an error!\n",
    "env.change_turn()\n",
    "env.act(action=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attribute `.available_actions` contains a numpy array of all not yet filled columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 3 4]\n"
     ]
    }
   ],
   "source": [
    "print(env.available_actions)\n",
    "# Column index '2' is missing because this column is already filled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Connect` class contains a method called `was_winning_move()` that checks whether the last move won the game (returns `True`) or not (returns `False`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winning move? False\n",
      "[[' ' ' ' 'o' ' ' ' ']\n",
      " [' ' ' ' 'x' ' ' ' ']\n",
      " [' ' ' ' 'o' 'x' ' ']]\n",
      "[[' ' ' ' 'o' ' ' ' ']\n",
      " [' ' ' ' 'x' ' ' ' ']\n",
      " [' ' 'o' 'o' 'x' ' ']]\n",
      "[[' ' ' ' 'o' ' ' ' ']\n",
      " [' ' ' ' 'x' 'x' ' ']\n",
      " [' ' 'o' 'o' 'x' ' ']]\n",
      "[[' ' ' ' 'o' ' ' ' ']\n",
      " [' ' ' ' 'x' 'x' ' ']\n",
      " ['o' 'o' 'o' 'x' ' ']]\n",
      "Player ' o ' has won the game!\n",
      "Winning move? True\n"
     ]
    }
   ],
   "source": [
    "# Obviously the game has not yet been won by any player.\n",
    "print(\"Winning move?\", env.was_winning_move()) \n",
    "\n",
    "# Make some moves\n",
    "env.act(action=3)\n",
    "env.change_turn()\n",
    "env.act(action=1)\n",
    "env.change_turn()\n",
    "env.act(action=3)\n",
    "env.change_turn()\n",
    "env.act(action=0)\n",
    "\n",
    "# Check again whether a player has won the game.\n",
    "print(\"Winning move?\", env.was_winning_move()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the `Connect` class contains a method called `grid_is_full()` that checks whether the grid still contains empty slots. You can use this method to check whether the game is a draw.\n",
    "\n",
    "Feel free to modify existing or add new methods to the `Connect` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Q-learning\n",
    "\n",
    "Your opponent is always the first player. Your agent is always the second player.\n",
    "\n",
    "For your reference, the pseudo-code for Q-learning is reproduced below from the textbook (Reinforcement Learning, Sutton & Barto, 1998, Section 6.5).\n",
    "<img src=\"images/q_learning.png\" style=\"width: 600px;\"/>\n",
    "\n",
    "Prepare a **learning curve** following the directions below. We refer to this as Plot 1.\n",
    "\n",
    "After $n$ steps of interaction with the environment, play $m$ games with the current policy of the agent (without modifying the policy). Think of this as interrupting the agent for a period of time to test how well it has learned so far. Your plot should show the total score obtained in these $m$ games as a function of $n, 2n, 3n, … kn$. The choices of $n$ and $k$ are up to you. They should be reasonable values that demonstrate the efficiency of the learning and how well the agent learns to play the game eventually. Use $m=10$. \n",
    "\n",
    "This plot should show the mean performance of $a$ agents, not the performance of a single agent. Because of the stochasticity in the environment, you will obtain two different learning curves from two different agents even though they are using exactly the same algorithm. We suggest setting $a$ to 30 or higher.\n",
    "\n",
    "Present a single mean learning curve with your choice of parameters $\\epsilon$ and $\\alpha$. The plot should also show (as a baseline) the mean performance of a random agent that does not learn but chooses actions uniformly randomly from among the legal actions. Label this line “Random Agent”. \n",
    "\n",
    "Please include this plot as a static figure in the appropriate cell below. You can look at the source code of this markdown cell to find out how to embed figures using html or you can use drag & drop. If you link to locally stored images, make sure to include those in your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1431aa87b9e9019a4dbe6e696e0a9082",
     "grade": true,
     "grade_id": "cell-3ac2114f764e8410",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win Percentage: 35.5 %\n",
      "Loss Percentage: 63.56 %\n",
      "Draw Percentage: 0.94 %\n"
     ]
    }
   ],
   "source": [
    "import connect\n",
    "import random\n",
    "import hashlib\n",
    "env = connect.Connect(verbose=False)\n",
    "\n",
    "#hash_object = hashlib.sha1(b'HelloWorld')\n",
    "#print(hash_object.hexdigest())\n",
    "\n",
    "def perform_random_move():\n",
    "    av = env.available_actions\n",
    "    env.act(av[random.randint(0, len(av) - 1)])\n",
    "\n",
    "def opponent_turn_and_won():\n",
    "    env.change_turn()\n",
    "    perform_random_move()\n",
    "    if(env.was_winning_move()):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def player_turn_and_won():\n",
    "    env.change_turn()\n",
    "    perform_random_move()\n",
    "    if(env.was_winning_move()):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# player = o\n",
    "# opponent = x\n",
    "\n",
    "games_won = 0\n",
    "games_lost = 0\n",
    "\n",
    "games = 5000\n",
    "\n",
    "for i in range(0,games): \n",
    "    env.reset(first_player='o')\n",
    "    \n",
    "    done = False\n",
    "    while(not done):\n",
    "        # if opponent didn't win then continue playing\n",
    "        if(not opponent_turn_and_won()):\n",
    "            # checks if grid if full, if not then continue playing\n",
    "            if(not env.grid_is_full()):\n",
    "                # if player has won then stop game\n",
    "                if(player_turn_and_won()):\n",
    "                    games_won += 1\n",
    "                    done = True\n",
    "            else:\n",
    "                # grid is full, so game is a draw\n",
    "                done = True\n",
    "        else:\n",
    "            # opponent has won, end game\n",
    "            games_lost += 1\n",
    "            done = True\n",
    "            \n",
    "print(\"Win Percentage:\", (100 * games_won/games),\"%\")\n",
    "print(\"Loss Percentage:\", (100 * games_lost/games),\"%\")\n",
    "print(\"Draw Percentage:\", (100 * (games - games_won - games_lost)/games), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "34a84d16a71c19e759cb0afc7b41bbbc",
     "grade": true,
     "grade_id": "cell-ce1405b859519f91",
     "locked": false,
     "points": 30,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "(A) [continued} Insert your static learning curve here (Plot 1).\n",
    "\n",
    "YOUR ANSWER HERE \n",
    "\n",
    "(B) In 3 sentences or less, explain your conclusions from the plot above. How close does your (average) agent get to the best possible level of performance? How efficiently does your (average) agent learn? \n",
    "\n",
    "YOUR ANSWER HERE \n",
    "\n",
    "(C) In five sentences or less, explain the key aspects of your implementation. How many state-action pairs do you represent in your Q-table? Describe and justify your settings of $\\alpha$ and $\\epsilon$. Are there any things you tried out that are not in your final implementation?\n",
    "\n",
    "YOUR ANSWER HERE\n",
    "\n",
    "(D) In the cell below, make it possible for us to produce from scratch a learning curve similar to Plot 1 but for a single agent, for a $k$ value of your own choosing. You do not need to include the baseline for random play.  This code should run in less than 30 seconds (ours runs in 2 seconds). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e65915a61d304027e4fbd2e714c4beba",
     "grade": true,
     "grade_id": "cell-e0e01e05236aee45",
     "locked": false,
     "points": 40,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### This cell should produce from scratch a plot showing a learning curve for a single agent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2A\n",
    " \n",
    "Using the minimax policy you computed, answer the following question: The first player (Player 1) drops his/her first disk into column 2 (counting from the left). Consider the resulting state, shown in the following code cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = connect.Connect(verbose=False)\n",
    "env.reset(first_player='o')\n",
    "env.act(action=1)\n",
    "print(env.grid[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the Minimax Value of this state for Player 2? That is, assuming an optimal opponent, does the Minimax Policy expect to win the game (value = 1), lose the game (value = –1), or end the game in a draw (value = 0)? Please state your answer as a number.    \n",
    "\n",
    "* The code cell below should compute this value and assign it to a variable called `state_value`.\n",
    "* Count the number of branches of the game tree that were examined and assign this number to a variable called `num_branches`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7b00948d6cc98e71a2f9467263067bc0",
     "grade": false,
     "grade_id": "cell-9e3a2a1bebc09565",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### Write all your code for Part 2A in or above this cell.\n",
    "\n",
    "# state_value = ...\n",
    "# num_branches = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0d9e96fd7c7ed9d9081eb564773f199c",
     "grade": true,
     "grade_id": "cell-c31d5222d21dd1d5",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograded test cell. Do not delete or change, otherwise you will get \n",
    "# no marks for this part of the assignment. Please make sure that this cell has \n",
    "# access to the variables state_value and num_branches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2B\n",
    "Plot a learning curve similar to the one in Part 1, comparing your Q-learning algorithm, random play, and Minimax play. Assume as before that the opponent always plays first and uses a random policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f6ed321958b16409ec1935b0fef7aa0d",
     "grade": true,
     "grade_id": "cell-a1d1652414bc7967",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### Write all your code for Exercise 2 (B) in or above this cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "614920eb9ba438cf5626dd59edc30252",
     "grade": true,
     "grade_id": "cell-1ea89dfffb81a041",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Explain your findings in 3 or fewer sentences. Which policy is better? Why?\n",
    "\n",
    "YOUR ANSWER HERE.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "\n",
    "Using your algorithm, compute the value of the game for your player (recall: your player goes second against a random opponent). The code cell below should compute this value and assign it to a variable called `optimal_policy_value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d7e4093f906a825959634f4ee6e6845f",
     "grade": false,
     "grade_id": "cell-a28f4f38b5cc6619",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### Write all your code for Part 3 in or above this cell.\n",
    "\n",
    "# optimal_policy_value = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b3a945d86dc159320c257d69389cf765",
     "grade": true,
     "grade_id": "cell-1e0341a7a580c299",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograded test cell. Do not delete or change, otherwise you will get \n",
    "# no marks for this part of the assignment. Please make sure that this cell has \n",
    "# access to the variable optimal_policy_value."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
