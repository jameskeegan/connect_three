{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect Three \n",
    "\n",
    "The primary description of this coursework is available on the CM20252 Moodle page. This is the Jupyter notebook you must complete and submit to receive marks. This notebook adds additional detail to the coursework specification but does not repeat the information that has already been provided there. \n",
    "\n",
    "You must follow all instructions given in this notebook precisely.\n",
    "\n",
    "Restart the kernel and run all cells before submitting the notebook. This will guarantee that we will be able to run your code for testing. Remember to save your work regularly.\n",
    "\n",
    "__You will develop players for Connect-Three on a grid that is 5 columns wide and 3 rows high. An example is shown below showing a win for Player Red.__\n",
    "\n",
    "<img src=\"images/connect3.png\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "For your reference, below is a visual depiction of the agent-environment interface in reinforcment learning. The interaction of the agent with its environments starts at decision stage $t=0$ with the observation of the current state $s_0$. (Notice that there is no reward at this initial stage.) The agent then chooses an action to execute at decision stage $t=1$. The environment responds by changing its state to $s_1$ and returning the numerical reward signal $r_1$. \n",
    "\n",
    "<img src=\"images/agent-environment.png\" style=\"width: 500px;\"/>\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "Below, we provide some code that will be useful for implementing parts of this interface. You are not obligated to use this code; please feel free to develop your own code from scratch. \n",
    "\n",
    "### Code details\n",
    "\n",
    "We provide a `Connect` class that you can use to simulate Connect-Three games. The following cells in this section will walk you through the basic usage of this class by playing a couple of games.\n",
    "\n",
    "We import the `connect` module and create a Connect-Three environmnet called `env`. The constructor method has one argument called `verbose`. If `verbose=True`, the `Connect` object will regularly print the progress of the game. This is useful for getting to know the provided code, debugging your code, or if you just want to play around. You will want to set `verbose=False` when you run hundreds of episodes to complete the marked exercises.\n",
    "\n",
    "This `Connect` environment uses the strings `'o'` and `'x'` instead of different disk colors in order to distuingish between the two players.\n",
    "\n",
    "Before we start a game, we should call the `reset()` method. This method cleans the board and resets other state variables. The `first_player` argument can be specified (`'o'` or `'x'`) to deterministically choose the starting player. It defaults to `\"random\"`, in which case each player starts the game with probability of $\\frac{1}{2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game has been reset.\n",
      "[[' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ']]\n"
     ]
    }
   ],
   "source": [
    "import connect\n",
    "env = connect.Connect(verbose=True)\n",
    "env.reset(first_player='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can interact with the environment using the `act()` method. This method takes an `action` as input and computes the response of the environment. An action is defined as the column index that a disk is dropped into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' 'o' ' ' ' ']]\n"
     ]
    }
   ],
   "source": [
    "env.act(action=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `act()` method inserts a disk into the specified column. \n",
    "\n",
    "If we want to change the player on move, we can do so by using the `change_turn()` method. We can check whose turn it is by accessing the `.player_at_turn` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current player at turn: o\n",
      "Current player at turn: x\n",
      "[[' ' ' ' ' ' ' ' ' ']\n",
      " [' ' ' ' 'x' ' ' ' ']\n",
      " [' ' ' ' 'o' ' ' ' ']]\n"
     ]
    }
   ],
   "source": [
    "print(\"Current player at turn:\", env.player_at_turn)\n",
    "env.change_turn()\n",
    "print(\"Current player at turn:\", env.player_at_turn)\n",
    "\n",
    "# Drop another disk into the same centre column.\n",
    "env.act(action=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we set `verbose=True`, the grid is printed each time we call the `act()` method. This grid is stored as a two-dimensional numpy array in the connect class and you can easily access by calling..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' 'o' ' ' ' ']\n",
      " [' ' ' ' 'x' ' ' ' ']\n",
      " [' ' ' ' ' ' ' ' ' ']]\n"
     ]
    }
   ],
   "source": [
    "current_grid = env.grid\n",
    "print(current_grid)\n",
    "# Notice that the grid now appears to be \"upside down\" because numpy arrays are printed from \"top to bottom\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[' ' ' ' 'o' ' ' ' ']\n",
      " [' ' ' ' 'x' ' ' ' ']\n",
      " [' ' ' ' 'o' ' ' ' ']]\n"
     ]
    }
   ],
   "source": [
    "# Let's make another move.\n",
    "env.change_turn()\n",
    "env.act(action=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we make another move with `act(action=2)`, the environment will throw an error because that column is already filled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for axis 0 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-3460a887c4c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# This cell should throw an error!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchange_turn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\GitHub\\connect_three\\connect.py\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0man\u001b[0m \u001b[0minteger\u001b[0m \u001b[0mreferring\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0mindex\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnew\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mdisk\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mdropped\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \"\"\"\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlowest_free_rows\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplayer_at_turn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlowest_free_rows\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlowest_free_rows\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 3 is out of bounds for axis 0 with size 3"
     ]
    }
   ],
   "source": [
    "# This cell should throw an error!\n",
    "env.change_turn()\n",
    "env.act(action=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attribute `.available_actions` contains a numpy array of all not yet filled columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 3 4]\n"
     ]
    }
   ],
   "source": [
    "print(env.available_actions)\n",
    "# Column index '2' is missing because this column is already filled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Connect` class contains a method called `was_winning_move()` that checks whether the last move won the game (returns `True`) or not (returns `False`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winning move? False\n",
      "[[' ' ' ' 'o' ' ' ' ']\n",
      " [' ' ' ' 'x' ' ' ' ']\n",
      " [' ' ' ' 'o' 'x' ' ']]\n",
      "[[' ' ' ' 'o' ' ' ' ']\n",
      " [' ' ' ' 'x' ' ' ' ']\n",
      " [' ' 'o' 'o' 'x' ' ']]\n",
      "[[' ' ' ' 'o' ' ' ' ']\n",
      " [' ' ' ' 'x' 'x' ' ']\n",
      " [' ' 'o' 'o' 'x' ' ']]\n",
      "[[' ' ' ' 'o' ' ' ' ']\n",
      " [' ' ' ' 'x' 'x' ' ']\n",
      " ['o' 'o' 'o' 'x' ' ']]\n",
      "Player ' o ' has won the game!\n",
      "Winning move? True\n"
     ]
    }
   ],
   "source": [
    "# Obviously the game has not yet been won by any player.\n",
    "print(\"Winning move?\", env.was_winning_move()) \n",
    "\n",
    "# Make some moves\n",
    "env.act(action=3)\n",
    "env.change_turn()\n",
    "env.act(action=1)\n",
    "env.change_turn()\n",
    "env.act(action=3)\n",
    "env.change_turn()\n",
    "env.act(action=0)\n",
    "\n",
    "# Check again whether a player has won the game.\n",
    "print(\"Winning move?\", env.was_winning_move()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the `Connect` class contains a method called `grid_is_full()` that checks whether the grid still contains empty slots. You can use this method to check whether the game is a draw.\n",
    "\n",
    "Feel free to modify existing or add new methods to the `Connect` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Q-learning\n",
    "\n",
    "Your opponent is always the first player. Your agent is always the second player.\n",
    "\n",
    "For your reference, the pseudo-code for Q-learning is reproduced below from the textbook (Reinforcement Learning, Sutton & Barto, 1998, Section 6.5).\n",
    "<img src=\"images/q_learning.png\" style=\"width: 600px;\"/>\n",
    "\n",
    "Prepare a **learning curve** following the directions below. We refer to this as Plot 1.\n",
    "\n",
    "After $n$ steps of interaction with the environment, play $m$ games with the current policy of the agent (without modifying the policy). Think of this as interrupting the agent for a period of time to test how well it has learned so far. Your plot should show the total score obtained in these $m$ games as a function of $n, 2n, 3n, … kn$. The choices of $n$ and $k$ are up to you. They should be reasonable values that demonstrate the efficiency of the learning and how well the agent learns to play the game eventually. Use $m=10$. \n",
    "\n",
    "This plot should show the mean performance of $a$ agents, not the performance of a single agent. Because of the stochasticity in the environment, you will obtain two different learning curves from two different agents even though they are using exactly the same algorithm. We suggest setting $a$ to 30 or higher.\n",
    "\n",
    "Present a single mean learning curve with your choice of parameters $\\epsilon$ and $\\alpha$. The plot should also show (as a baseline) the mean performance of a random agent that does not learn but chooses actions uniformly randomly from among the legal actions. Label this line “Random Agent”. \n",
    "\n",
    "Please include this plot as a static figure in the appropriate cell below. You can look at the source code of this markdown cell to find out how to embed figures using html or you can use drag & drop. If you link to locally stored images, make sure to include those in your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1431aa87b9e9019a4dbe6e696e0a9082",
     "grade": true,
     "grade_id": "cell-3ac2114f764e8410",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import connect\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "env = connect.Connect(verbose=False)\n",
    "q = {}\n",
    "alpha= 0.99\n",
    "gamma = 0.01\n",
    "games = 1\n",
    "#gamma_ch = 0.98/games\n",
    "epsilon = 5\n",
    "#change = 0.98 / (games)\n",
    "a = 0\n",
    "\n",
    "def string_state(state):\n",
    "    \n",
    "    \"\"\"Turns current game grid into a string\"\"\"\n",
    "    \n",
    "    stringed = \"\"\n",
    "    \n",
    "    for i in range(0,3):\n",
    "        for j in range(0,5):\n",
    "            stringed += state[i,j]\n",
    "            \n",
    "    return stringed\n",
    "\n",
    "def get_max(state_actions):\n",
    "    \"\"\"Gets the maximum possible action value for part of Q-learning equation\"\"\"\n",
    "    s = string_state(np.copy(env.grid))\n",
    "    \n",
    "    # makes entry for this state if one doesn't already exist\n",
    "    if s not in q:\n",
    "        #new_state(s,state_actions)\n",
    "        #print(\"new get_max state\")\n",
    "        #print(s)\n",
    "        return 0\n",
    "    else:\n",
    "        avs = q[s]\n",
    "\n",
    "        highest = -10000\n",
    "\n",
    "        # finds highest possible action value, returns this\n",
    "        for i in avs:\n",
    "            if avs[i] > highest:\n",
    "                highest = avs[i]\n",
    "                #print(\"highest=\",avs[i])\n",
    "\n",
    "        return highest\n",
    "    \n",
    "def get_greedy():\n",
    "    state_actions = np.copy(env.available_actions)\n",
    "    s = string_state(np.copy(env.grid))\n",
    "    if s not in q: new_state(s,state_actions)\n",
    "        \n",
    "    r = random.randint(0,epsilon)\n",
    "    \n",
    "    #if r==0: return random.randint(0, len(state_actions)-1)\n",
    "    \n",
    "    avs = q[s]\n",
    "    highest = -10000\n",
    "    choice = 0\n",
    "\n",
    "    iteration = 0\n",
    "    for i in avs:\n",
    "        print(\"avs[i]\",avs[i])\n",
    "        if avs[i] > highest:\n",
    "            highest = avs[i]\n",
    "            choice = i\n",
    "            #print(\"choice now:\",choice)\n",
    "        iteration += 1\n",
    "        \n",
    "    return choice\n",
    "                  \n",
    "    \n",
    "def new_state(s, state_actions):\n",
    "    \"\"\"Updates q-table with new state when a new state is found\"\"\"\n",
    "    #state_actions = np.copy(env.available_actions)\n",
    "    \n",
    "    # l will contain all the possible actions that can be taken from this state\n",
    "    l = {}\n",
    "    for i in range(0, len(state_actions)):\n",
    "        j = str(state_actions[i])\n",
    "        l[j] = 0.0\n",
    "    q[s] = l\n",
    "\n",
    "def update_q(state,last_action,reward,state_actions):\n",
    "    \"\"\"Updates value in the q table using the given Q-learning equation\"\"\"\n",
    "    \n",
    "    global alpha    \n",
    "    global gamma\n",
    "    \n",
    "    # stringifies the state\n",
    "    s = string_state(state)\n",
    "    \n",
    "    # makes entry for this state if one doesn't already exist\n",
    "    if s not in q: new_state(s,state_actions)\n",
    "        \n",
    "    # gets list of possible actions for the given state\n",
    "    avs = q[s]\n",
    "    # gets the max a part of the equation\n",
    "    max_a = get_max(state_actions)\n",
    "    \n",
    "    # updates value in q-table\n",
    "    a = str(last_action)\n",
    "    \n",
    "    if a not in avs: print(\"huh\")\n",
    "        \n",
    "    avs[a] += float(alpha * (reward + gamma * max_a - avs[a]))\n",
    "\n",
    "def perform_random_move():\n",
    "    \"\"\"Performs a random move for whatever player called it\"\"\"\n",
    "    av = env.available_actions\n",
    "    action = random.randint(0, len(av) - 1)\n",
    "    env.act(av[action])\n",
    "    \n",
    "def get_best(state):\n",
    "    s = string_state(state)\n",
    "    \n",
    "    # makes entry for this state if one doesn't already exist\n",
    "    if s not in q: \n",
    "        new_state(s,np.copy(env.available_actions))\n",
    "        #print(\"made state\")\n",
    "    \n",
    "    avs = q[s]\n",
    "\n",
    "    highest = -10000\n",
    "    \n",
    "    choice = 0\n",
    "\n",
    "    #print(\"avs:\", avs)\n",
    "    #print(\"env.aa:\", env.available_actions)\n",
    "    #print(s)\n",
    "    \n",
    "    # finds highest possible action value, returns this\n",
    "    for i in avs:\n",
    "        #print(\"i:\",i)\n",
    "        if i in avs:\n",
    "            #print(\"avs[\",i,\"] =\",avs[str(i)])\n",
    "            if avs[i] > highest:\n",
    "                highest = avs[i]\n",
    "                choice = i\n",
    "                #print(\"choice now:\",choice)\n",
    "            \n",
    "    #print(\"choice:\", choice)\n",
    "\n",
    "    return choice\n",
    "            \n",
    "    \n",
    "def player_turn(testing):\n",
    "    \"\"\"Code to dictate the turn for the player\"\"\"\n",
    "    env.change_turn()\n",
    "    \n",
    "    state = np.copy(env.grid)\n",
    "    \n",
    "    #gets available actions\n",
    "    av = np.copy(env.available_actions)\n",
    "    \n",
    "    if not testing:\n",
    "        # not testing, so chooses random action\n",
    "        action = random.randint(0, len(av) - 1)\n",
    "        env.act(av[action])\n",
    "        return av[action]\n",
    "    else:\n",
    "        #action = random.randint(0, len(av) - 1)\n",
    "        action = int(get_best(state))\n",
    "        #print(\"action:\",action)\n",
    "        env.act(action)\n",
    "        return action\n",
    "    \n",
    "def opponent_turn():\n",
    "    env.change_turn()\n",
    "    perform_random_move()\n",
    "\n",
    "def play_turn_and_done(testing):\n",
    "    # check if grid is full before performing player term\n",
    "    \n",
    "    reward = 0\n",
    "    \n",
    "    # grid is full so end game\n",
    "    if env.grid_is_full(): return 0\n",
    "    \n",
    "    state = np.copy(env.grid)\n",
    "    state_actions = np.copy(env.available_actions)\n",
    "    last_action = player_turn(testing)\n",
    "    \n",
    "    # if player did winning turn then return 1 (for win)\n",
    "    if(env.was_winning_move()):\n",
    "        reward = 1\n",
    "    else:\n",
    "        # opponent takes their random turn\n",
    "        opponent_turn()\n",
    "    \n",
    "        # if opponent bot won then return -1 (for loss)\n",
    "        if(env.was_winning_move()):\n",
    "            reward = -1\n",
    "    \n",
    "    # updates q table if we're in training phase\n",
    "    if not testing: update_q(state,last_action,reward,state_actions)\n",
    "    \n",
    "    # returns win or loss if there was one, else -2 to indicate that game is still  playing\n",
    "    if reward == 1 or reward == -1:\n",
    "        return reward\n",
    "    \n",
    "    return -2\n",
    "    \n",
    "\n",
    "def play_game(testing):\n",
    "    env.reset(first_player='o')\n",
    "    done = False\n",
    "    perform_random_move()\n",
    "    while True:\n",
    "        ret = play_turn_and_done(testing)\n",
    "        if ret == 1 or ret == 0 or ret == -1:\n",
    "            return ret\n",
    "        \n",
    "def play_rand():\n",
    "    env.reset(first_player='o')\n",
    "    done = False\n",
    "    while True:\n",
    "        if env.grid_is_full(): return 0 \n",
    "        opponent_turn()\n",
    "        if env.was_winning_move():\n",
    "            return -1\n",
    "        else:\n",
    "            env.change_turn()\n",
    "            if env.grid_is_full(): return 0\n",
    "            perform_random_move()\n",
    "            if env.was_winning_move():\n",
    "                return 1\n",
    "            \n",
    "def reset():\n",
    "    q = {}\n",
    "    print(q)\n",
    "        \n",
    "def test(random):\n",
    "    \"\"\"Game plan for testing the player\"\"\"\n",
    "    m = 10\n",
    "    \n",
    "    acc = 0\n",
    "    wins = 0\n",
    "    for i in range(0,a):\n",
    "        for j in range(0,m):\n",
    "            r = 0\n",
    "            if random:\n",
    "                r = play_rand()\n",
    "            else:\n",
    "                r = play_game(testing=True)\n",
    "            \n",
    "            acc += r\n",
    "            if r == 1: wins += 1\n",
    "\n",
    "    return (acc/(a)), (100 * wins/(a*m))\n",
    "\n",
    "def game(no_of_games, n, agents, show_random, graph_for_wins):\n",
    "    print(\"Playing game...\")\n",
    "    \n",
    "    global games\n",
    "    games = no_of_games\n",
    "    \n",
    "    reset()\n",
    "    \n",
    "    global alpha\n",
    "    global gamma\n",
    "    \n",
    "    global a\n",
    "    a = agents\n",
    "    \n",
    "    alpha = 0.7\n",
    "    gamma = 0.05\n",
    "    \n",
    "    k = int(games/n)\n",
    "    \n",
    "    ep_inc = int(95 / k)\n",
    "    gamma_inc = 0.98/k\n",
    "    \n",
    "    results = np.empty(k)\n",
    "    rand_results = np.empty(k)\n",
    "    \n",
    "    results_won = np.empty(k)\n",
    "    rand_results_won = np.empty(k)\n",
    "    \n",
    "    results[0], results_won[0] = test(random=True)\n",
    "    if show_random: rand_results[0], rand_results_won[0] = test(random=True)\n",
    "\n",
    "    for i in range(1,k):\n",
    "        for j in range(1,n):\n",
    "            play_game(testing=False)\n",
    "        results[i], results_won[i] = test(random=False)\n",
    "        if show_random: rand_results[i], rand_results_won[i] = test(random=True)\n",
    "        global epsilon\n",
    "        epsilon += ep_inc\n",
    "    \n",
    "    #q = {}\n",
    "    \n",
    "    if graph_for_wins:\n",
    "        plt.plot(results_won, label = \"AI\")\n",
    "        if show_random : plt.plot(rand_results_won, label = \"Random\")\n",
    "        plt.ylabel(\"Win percentage\") \n",
    "        plt.yticks(np.arange(20, 100, 10))\n",
    "    else:\n",
    "        plt.plot(results, label = \"AI\")\n",
    "        if show_random : plt.plot(rand_results, label = \"Random\")\n",
    "        plt.ylabel(\"Total score obtained by m\") \n",
    "        plt.yticks(np.arange(-10, 10, 2))\n",
    "        \n",
    "    title = str(a) + \" agents, n=\" + str(n) + \", m=\" + str(10) + \", games=\" + str(games)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Games played (in thousands)\")\n",
    "    \n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1)\n",
    "    \n",
    "    ax1.plot(results, label = \"AI\")\n",
    "    ax1.plot(rand_results, label = \"Random\")\n",
    "    ax1.set_title(\"30 agents, n=1000, m=10, games = 100000\")\n",
    "    ax1.set_xlabel(\"Games played (in thousands)\")\n",
    "    ax1.set_ylabel(\"Total score by m agents\")\n",
    "    \n",
    "    ax2.plot(results_won, label = \"AI\")\n",
    "    ax2.plot(rand_results_won, label = \"Random\")\n",
    "    ax2.set_xlabel(\"Games played (in thousands)\")\n",
    "    ax2.set_ylabel(\"Win percentage\")\n",
    "    ax2.set_xticks(20, 100, 10)\n",
    "    #plt.axis([0,k,-10,10]) # Put in side your range [xmin,xmax,ymin,ymax], like ax.axis([-5,5,-5,200])\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing game...\n",
      "{}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-6ea9bbee535e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mgame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_random\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_for_wins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Produced graph in\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"seconds\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-b902d928dbd1>\u001b[0m in \u001b[0;36mgame\u001b[1;34m(no_of_games, n, agents, show_random, graph_for_wins)\u001b[0m\n\u001b[0;32m    285\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m             \u001b[0mplay_game\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtesting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m         \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresults_won\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mshow_random\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mrand_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrand_results_won\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-b902d928dbd1>\u001b[0m in \u001b[0;36mplay_game\u001b[1;34m(testing)\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[0mperform_random_move\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplay_turn_and_done\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtesting\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    213\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mret\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mret\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-b902d928dbd1>\u001b[0m in \u001b[0;36mplay_turn_and_done\u001b[1;34m(testing)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;31m# if player did winning turn then return 1 (for win)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwas_winning_move\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m         \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\connect_three\\connect.py\u001b[0m in \u001b[0;36mwas_winning_move\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mcol_candidates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_row\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_connect\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_row\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_connect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_col\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch_sequence_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol_candidates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwinning_sequence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m                 \u001b[0mgame_is_won\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\connect_three\\utils.py\u001b[0m in \u001b[0;36msearch_sequence_numpy\u001b[1;34m(arr, seq)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;31m# Create 2D array of sliding indices across entire length of input array.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;31m# Match up with the input sequence & get the matching starting indices.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m     \u001b[0mM\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNa\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mNseq\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mr_seq\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mseq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "games = 10000\n",
    "n = 1000\n",
    "agents = 30\n",
    "show_random = True\n",
    "graph_for_wins = False\n",
    "\n",
    "start = time.time()\n",
    "game(games, n, agents, show_random, graph_for_wins)\n",
    "print(\"Produced graph in\",(time.time() - start),\"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "34a84d16a71c19e759cb0afc7b41bbbc",
     "grade": true,
     "grade_id": "cell-ce1405b859519f91",
     "locked": false,
     "points": 30,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "(A) [continued} Insert your static learning curve here (Plot 1).\n",
    "\n",
    "<img src=\"plot_best.png\" style=\"width: 600px;\"/>\n",
    "\n",
    "(B) In 3 sentences or less, explain your conclusions from the plot above. How close does your (average) agent get to the best possible level of performance? How efficiently does your (average) agent learn? \n",
    "\n",
    "YOUR ANSWER HERE \n",
    "\n",
    "(C) In five sentences or less, explain the key aspects of your implementation. How many state-action pairs do you represent in your Q-table? Describe and justify your settings of $\\alpha$ and $\\epsilon$. Are there any things you tried out that are not in your final implementation?\n",
    "\n",
    "YOUR ANSWER HERE\n",
    "\n",
    "(D) In the cell below, make it possible for us to produce from scratch a learning curve similar to Plot 1 but for a single agent, for a $k$ value of your own choosing. You do not need to include the baseline for random play.  This code should run in less than 30 seconds (ours runs in 2 seconds). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e65915a61d304027e4fbd2e714c4beba",
     "grade": true,
     "grade_id": "cell-e0e01e05236aee45",
     "locked": false,
     "points": 40,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing game...\n",
      "{}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3XecVPX1//HXG5AuIEWQXu0iyAKiImCJNZYUC9hFsJDElJ9G0/NNMSYxGisdFFCxm6ixUkSlVxFFdul16Z1t5/fHvavjujs7wE7d83w85sHMLXPPDLNz5n7u53M+MjOcc865slRJdgDOOedSmycK55xzUXmicM45F5UnCuecc1F5onDOOReVJwrnnHNReaJwzjkXlSeKJJA0RNJsSQckjUl2PKWR1FfSmmTHEUnSZEn7Je0Ob1+UWN9f0kpJeyS9KqlhxLqGkl4J162U1D/WfZNB0smS3pa0WdK3BjuV93oqG0k3SjJJAyOWSdLfJG0Jbw9KUsT6LpLmSNob/tulIvbNRJ4okmMd8CdgVLIDSUNDzKxueDuueKGkk4ChwPVAU2Av8ETEfo8DeeG6AcCT4T6x7JsM+cBE4NYy1pf5eiobSUcB9wGLS6waBFwBnAp0Bi4FBof7VAdeA8YBRwFjgdfC5Ye7b+YxM78l6UaQLMaUs00H4ANgC7AZGA80iFh/GjAP2AW8ADwP/Cli/aXAfGA78DHQOWLdCuAXwEJgR7hvTaAOsA8oAnaHt+ZAD2A2sBPYCDwU4+v8PcGX3tNhnIuBrEN4vyYDA8tY9xdgQon3LQ84Mnw9ecCxEeufAR4ob98YYuoLrAHuATYB6wm+YC4GlgJbgfsP4zPSMfgz/cayqK8nhudsB0wN/y/eI0g64yLWvwBsCD8TU4GTItaNIUiib4Wfi4+AZsDDwDbgc6BrxPbNgZeAXGA58OOIdYf0eSrl9TwF3Fny8xF+3gdFPL4VmB7e/w6wFlDE+lXAhYe7bybe/Iwi9Qn4K8Ef3AlAK4Iv3uJfNq8Q/PE2BJ4FrvxqR+k0grOWwUAjgl/Nr0uqEfH8VwEXEnx5dAZuMrM9wEXAOvv61/s64BHgETOrR/BlOvEgXsdlwHNAA+B14LGIOP8raXsZt/+WeJ6/hs0xH0nqG7H8JGBB8QMzyyb8Mg1vhWa2NGL7BeE+5e0bi2YECbYF8FtgOHAd0A3oDfxWUvvwtfaP8lq3S2odw/HKez3lmQDMJPhM/J7gTCrSW0An4GhgLsGPk0hXAb8GGgMHgE/C7RoDLwIPha+1CvCfMLYWwLnA3ZIuCJ+nzM9TOe/RLyO26wFkESSLkr7x/8q3/88XWvgtH1pIGZ+Jg9w341RLdgAuOjNbBiwLH+ZKegj4Xfj4dIL/w3+HH9qXJc2M2P02YKiZzQgfj5V0f7jflHDZv8MkgKT/ANHaWvOBjpIam9lmYPpBvJRpZvZmeJxngLsjXuOlMT7HvcBnBF/i1wD/kdQl/GKvS/ALONIOgjOKwijrKGffWOQDfzazQknPAcMIvgB3AYslLSZIwjlmNoHgi/pwHHK8YSLqDpxrZnnANEmvR25jZqMitv89sE1SfTMrPuYrZjYnXP8KcKeZPR0+fh4YEm7XHWhiZn8MH+dIGk7wf/c2UT5PZtYghtdSleDs5kdmVhRxCaFYyfdpB1A3vNZQ3nt4OPtmHD+jSHGSjpb0nKS1knYStIs2Dlc3B9aW+GWzOuJ+G+Dnkb/GCM5ImkdssyHi/l6CP4Ky3Erwa/ZzSbMkxfoFX9pxako6qB8qZjbDzHaZ2QEzG0vQ7HFxuHo3UK/ELvUImleirStv31hsMbPC8P6+8N+NEev3Ef19PViHE29zYKuZ7Y1Y9tVnRlJVSQ9Iyg4/byvCVY0jti/52sp6rW2A5iU+f/cTXFeBw/s8QdDctNDMPiljfcn3qR6wO/x7OdjPxMHsm3E8UaS+vwJGcG2hHkGTRvFPp/VAC33zp1SriPurCX7pNoi41TazZ2M47rd62pjZl2Z2LUGTxN+AFyXVOYTX9A2S3tLXPZlK3t4qJ8bi176Y4MJj8XO2B2oQXCdYClST1Cli31P5+uJntH0rlKQBUV7r7hibnsp7PdGsBxpKqh2xLPIz0x+4HDgPqA+0LQ49hucuaTWwvMTn70gzuxiif57KeY/uD5//XOBKSRskbQDOAP4pqbhZ8xv/r3z7/7xzib+dzpTxmTjIfTOOJ4okkFRNUk2gKlBVUrRf10cS/ILZLqkF8P8i1n1C0KwyJHzOywkuEBYbDtwuqacCdSRdIimWU+SNQCNJ9SPivk5SEzMrIrg4Tnh8JK2QdFMMz/stZnZRxLWQkreLwudvIOmC4vdK0gDgbIImDAja0b8rqXf4ZfNH4OXwDGQP8DLwx/A9OJPgy/CZ8vYNjz1GFdSN2czGR3mtdc1sVXhMhZ+R6uHjmgqvLZX3eiS1VdBVtG0px19JcAH595KqS+oFfDdikyMJrjtsAWoTXOg/VDOBnZLulVQrPFs5WVL3MM4yP0/lvEfFMd1EcN2uS3ibDfwB+FW4/mngZ5JaSGoO/Jzgeh4EF74LgR9LqiGpuLnsgwrYN/Mk+2p6ZbwRXEC0Erffl7HtScAcgmQxn+ADuyZifVa4fDdBb5WXgd9ErL8QmEXwh7g+3ObIcN0K4LwScUX2fhlF8IWxnaDJYhxBz57dBL+ergi3q05w2n18lNcb+bxtw9dc7SDesybh69gVxjMdOL/ENv0Jep/sIei+2DBiXUPg1XDdKqD/Qez7PnBbGXH1LfH/US18bW0jlk0DrjvIz0jxexR5WxHL6yG4gL4COKKM5+4AfBi+l+8TXFMZGa6rG77+XcBK4Ibw2B3D9WP4Zq+6gcDkiMcdgYKIx80JOllsIOgVNb34M1fW5+kw/q4m881eTwIeJOh5tjW8H9lTqSvB39Y+govxXSti30y8KXzRLkNImgE8ZWajE3jMs4C7LGhGyCgKepYtIGj6y092PLGQ9Gsg18yGxrj988DnZva7cjd2lZInijQnqQ/wBcEYiwEE3QTbm9n6pAbmUlbY9LOVYFzDdwjOTHqZ2bykBuZSlnePTX/HEfQ/rwtkAz/wJOHK0YygibIRwWDBOzxJuGj8jMI551xU3uvJOedcVBnR9NS4cWNr27ZtssNwzrm0MmfOnM1m1qS87TIiUbRt25bZs2cnOwznnEsrklbGsp03PTnnnIvKE4VzzrmoPFE455yLyhOFc865qDxROOeci8oThXNpZs22vQyfmsOmXfuTHYpLIjPjH29/wadrS86hVPEyonusc5VFTu5u+g+fwYad+/nHO19wTfdWDOrTgRYNaiU7NJdARUXGb177lPEzVgFwcov65exxePyMwrk08cWGXVw1dDr5hUWMuCGLK7q0YPyMVfT9+yTufXEhKzbvSXaILgEKi4x7XlrI+BmruL1PB37+nVindj90GVHrKSsry3zAnctkn67dwfUjZ3BE1SpMuK0nHY8O5p5au30fw6Zk8+ys1RQUFnFp5+bc1a8jxzXL2OmbK7X8wiJ+NnEB/1mwjrvP68RPzu2Evj1XeMwkzTGzrHK380ThXGqbt2obN46ayZE1j2D8wJ60bfzt2Wc37drPyA+XM276SvbkFfKdE5sy5JyOdG7ZIAkRu3g4UFDIj5+dx9uLN/LLi47n9j4dDvs5PVE4lwFmLt/KzaNn0vjIGowf2JOWR9WOuv22PXmM/ngFYz5azs79BZx9bBOG9OtIj3YNExSxi4f9+YXcPm4Ok7/I5fffPZGbzmxXIc/ricK5NDfty83c9vRsmjeoyYTbTqdpvZox77trfz7jpq9ixIc5bNmTR492DRnSryO9OzU+rKYKl3h78woYOHY2n+Rs4S9XnsK1PVpX2HN7onAujX3w+UZuHzeX9o3rMG5gTxrXrXFIz7Mvr5DnZq1i6JQcNuzcz6kt63NXv46cd0JTqlTxhJHqdu3P5+bRs5i7ahv/vOpUruzaskKf3xOFc2nqf5+u50fPzuP4ZvV45tYeNKhd/bCf80BBIS/PXcuTk7NZtXUvxzc7kjv7deSSU46hqieMlLR9bx43jprJ4nU7eeSarlzS+ZgKP4YnCufS0Gvz1/KziQvo0qoBo2/uTr2aR1To8xcUFvGfhet4fFI2yzbtpl3jOtzRtwNXdm3BEVW9t3yq2LL7ANeNnEn2pt08MeA0zjuxaVyO44nCuTQzcdZq7n15IT3bNWTkjd2pUyN+42GLioy3F2/gsUnLWLxuJy0a1GJwn/ZcldWKmkdUjdtxXfk27dxP/xEzWLNtL8Ouz+LsY8udV+iQeaJwLo08/ckKfvvaYs4+tglDr+tGreqJ+bI2MyYvzeWxD5YxZ+U2mhxZg9t6t2NAzzZxTVSudGu372PA8Ols2nWAUTd15/T2jeJ6PE8UzqWJ4VNz+PObSzjvhKY8PqArNaol/he9mTE9ZyuPT1rGtGWbaVD7CG45sx03ntGW+rUqtvnLlW7Vlr1cO3w6O/flM+aWHnRrc1Tcj+mJwrk08Oj7X/LPd5dyySnH8PA1XVLiOsG8Vdt4fNIy3luyibo1qnFDrzbcelY7Gh1izytXvuzc3QwYPoP9BYU8c0tPTmkZ39pNxTxROJfCzIx/vPMFj0/K5nunteDB73emWgokiUifrdvJ45OX8eai9dSoVoX+Pdow6Oz2NKsf+3gOV74vNuxiwIgZgDFuYE+Ob1YvYcf2ROFcijIz/vTGEkZOW861PVrz5ytOTukxDcs27ebJydm8On8tVSW+360ld/TpQOtG0UeJu/IV1/CqXq0K4weeTsej6yb0+J4onEtBkeWhbzqjLb/77olpM1J69da9PDUlmxdmr6HQjMtPbc6d/Tp8VaDQHZy5YQ2vejWPYMJtPWnT6Ns1vOLNE4VzKaawyLj3pYW8OGcNd/TtwD0XHJc2SSLSxp37GT41h/EzVrG/oJCLTm7GnX07xn1OhEwyI2cLt4yZReMjazDhttOTNp+IJwrnUkh+YRE/fX4+/124np+edyw/PrdjWiaJSFv35DFq2nLGfryCXQcK6HdcE4ac0ykhvXXS2Ydf5nLb07Np0aDWQdfwqmhpnygk/RQYCBiwCLjZzEqd+9EThUtlBwoK+dGEebzz2Ubuu+h4BldAeehUsmNfPs98soKR05azbW8+vdo34kfndKRXh0Zpnwwr2vtLNnLHuLm0b3J4NbwqSlonCkktgGnAiWa2T9JE4E0zG1Pa9p4oXKqKLA/9h8tO4sYz2iY7pLjZm1fAhBmrGDY1h027DtC1dQOG9OvIOccf7QkDeHPRen787DxObF6Pp2+pmBpehyvWRJFa/fG+qRpQS1I1oDawLsnxuIOweuteHn5vKau27E12KEmzN6+AW8bMYsrSXB743ikZnSQAalevxsDe7Zl6Tz/+dMXJbNp5gFvHzuaSf0/jjYXrKSxKvR+lifLqvLUMmTCXU1s1YNzAnimRJA5GSp5RAEj6CfBnYB/wjpkNKLF+EDAIoHXr1t1WrlyZ+CBdqYoHD23YuZ+qVcRlpzbnzr4d6NS08vSOiXd56HSQX1jEa/PX8cTkZeTk7qFDkzrc2bcjl3VpnhIDCxPl+Vmr+OXLizi9XSNG3JiVUqVR0r3p6SjgJeBqYDvwAvCimY0rbXtvekodkYOHHr66K1OWbmLc9KB3zIUnNeOufpnfOyayPPS/r+3KxadUfHnodFJYZLz16Xoe+2AZn2/YRauGtbi9Twd+0K1lUsqVJFJxDa8+xzZh6PXdUq7gYronih8CF5rZreHjG4DTzezO0rb3RJEayho8tHVPHqM/Ws6YjyJ7x3SkW5vMm54zUeWh05GZ8f6STTw2aRnzV2+nab0aDDq7A9f2aEXt6qnzK7uiDJuazV/e/JzzT2zKY/2TU8OrPOmeKHoCo4DuBE1PY4DZZvZoadt7oki+WAYP7dyfzzOfrGTEhzlf9Y4Zck5HzsiQ3jGR5aGH35BF707xKw+dzsyMj5Zt4bFJXzI9ZysN61Tn1rPacX2vNhU+/0YymBmPfrCMh95dyqWdj+FfV6dGDa/SpHWiAJD0B4KmpwJgHjDQzA6Utq0niuQ62MFDmdg7prg8dO6uA4xMQHnoTDF7xVYem7SMyV/kcmTNatx8RltuPrMdR9VJr4u9xcyMv7/9BU9Mzub7p7XkwR90TukZBNM+URwMTxTJUzx4qOVRtRk/sOdBDR7an1/Ii3PW8NSUbNZs28cJx9RjSL+OXHhys5T+4yrpq/LQ+/MZc3NiykNnmkVrdvD4pGX8b/EGalevyoCerbmtd3uOTuJgtINlZvzxv58x+qMVDOjZmv+7PLVreIEnCpcA7y/ZyB3j59K+8eENHirZO6Z9kzrclSa9Y7Jzd9N/+HQOFBQx7taeGX+hPt6+3LiLJyZn89r8tVSrWoWrs1oxuE97Wh6V2gUIi4qMX7/2KRNmrOKWM9vxm0tPSIuzY08ULq7eWrSeH1Xw4KGSvWNaHlWLO/qmbu+Yzzfs5LoRMwASXh46063csoenpmTz4pw1mMGVXVtwR98OtG+S2OqqsSgoLOKelxby8ty13Nm3A/8vjWp4eaJwcfPqvLX8/IUFdGnVgNE3d6/wC5Bmxgefb+LRD77uHXNb7/b079k6ZXrHfLp2B9eNnEGNJJWHrizWbd/HsKk5PDtzFfmFRVzSuTl39euQMkk5v7CIu5+fzxsL1/Pz84/lR+d2SnZIB8UThYuLRA4eMjM+zt7Cox+kVu+YVCgPXdnk7jrAyGnLeeaTFezJK+S8E5oy5JyOdGnVIGkxHSgoZMiEebz72Ubuv/h4Bp2dfjW8PFG4CpfMwUNzVm7lsQ+WMSnsHXNT2DumYYJ7xxT38GpyZA3GJ7E8dGW1fW8eYz5eweiPVrBjXz69OzVmSL+O9ExwL7P9+YUMfmYOU5bm8sfLT+KGXm0TevyK4onCVahUGTz06dqve8fUrBb0jhl0dmJ6x0T28JowsGda9cjJNLsPFDB++kqGf5jD5t15dG97FHf160ifY5vE/frAngMFDBw7m+nLt/DA907h6u6t43q8ePJE4SpEqg4eKu4d8/qCdVStIq7KasngszvQqmF8escUl4fucHRdnrm1R9LLQ7vA/vxCnp+1mqemZLN+x35OaVGfu/p15DsnNo1L19SdYQ2v+au3888fnsoVXVtU+DESyROFO2zpMHioZO+YK7q24M4K7h1TXB76pOb1GJsi5aHdN+UVFPHKvDU8MTmblVv2cmzTutzVryOXnHIM1Sroh832vXncMGomn63byaPXduWiDKjh5YnCHZbIwUP9e7bmTyk+eGj9jq97xxwoKOKSU47hrn4dOeGYw+sd8+q8tfxs4nxOa30Uo+LQw8tVrILCIt5YtJ7HJy1j6cbdtGlUmzv7duDKri2pXu3QE8bm3Qe4bsQMcjbv4anrTuOc4zOjhpcnCnfIIgcP3XxmW3576Ylp0y988+7i3jEr2X2g4LB6x6RyeWgXXVGR8c5nG3l80jIWrd1B8/o1GdynA1d3b3XQnTA27tzPgLCG14gbunNWp8ZxijrxPFG4Q1JYZNzz4kJemrsm7QYPRdqxN58xH69g1EfLv+odc1e/jvRs1zCm1zP24xX87vXULQ/tYmNmTFmay+OTljFrxTYa163Bbb3bMeD0NtSNIfGv2baXASNmsHnXAUbd1D3hvavirUIThaQTgbYEs84BYGavH06AFckTRcXILyzip8/P579pOnioNF/3jlnO5t0HyGpzFEPOid47ZuiUbP76VvJ7eLmKNSNnC49NWsaHX26mfq0juPnMttx8Rjvq1y69OXHllj30Hz6DnfvzGXtLD05rnXk1vCosUUgaDmQBnwFF4WIzsxsOO8rox20AjABOBgy4xcw+KW1bTxSHLxMGD0VT3Dtm6JRs1pXRO8bM+Pf7y/jXe6nVw8tVrPmrt/PYB8t4b8lG6taoxnWnt2Fg73bf6Mm2bNNuBoyYTl5BEc9kcA2vikwUS4ATLcFtVJLGAh+a2QhJ1YHaZra9tG09URyeTBk8FIvi3jFPTs5mRYneMf98dylPpnAPL1exlqzfyeOTlvHGovXUqFaFa7q3ZnCf9uzYlx/W8BLjB/bkuGaZO4VvRSaKMcBfzeyLCoqtXJLqAQuA9rEkKE8Uhy6TBg8djJK9YxrWqc7WPXlpUx7aVZyc3N08OTmbV+atRYKa1apSp0Y1xt/Wkw4pWISwIlVkougN/AdYCxwARND0dFpFBFrGMbsAwwiau04F5gA/MbM9EdsMAgYBtG7dutvKlSvjFU7G2rk/n1tGz2JehgweOhRFRca7SzYy4sMcsto25J40vXjvDt+abXsZOiWHxet28PDVXWndKLVLm1eEikwUXwL3Aov4+hoFZpZ9uEFGOWYWMB0408xmSHoE2Glmvyltez+jOHiZOHjIOXdwYk0UsXQMX21mL1dATAdjDbDGzGaEj18EfpngGDLWV4OHcvcw9PpunHtCZgwecs7FRyyJ4jNJTxM0P301Z3U8u8ea2QZJqyUdF14bOZegGcodpsjBQyNvyqJ3pybJDsk5l+JiSRTF/cIui1hmQLzHUfwIGB/2eMoBbo7z8TLe2u376D98Opt3HWDszT0ybvCQcy4+yk0UZnZ9IgIp5bjzCcZvuAoQOXjomYE9M3LwkHMuPrx4TSUQOXjo2dtOz9jBQ865+PBEkeE+37Dzq8FDzw3qldGDh5xz8VFufQJ5p/K0tWjNDq4ZNp1qVarw/ODTPUk45w5JLIVssiX9VdKxcY/GVZg5K7fRf8R06taoxsTBvTJ+hKlzLn5iSRRdgVXAOEnTJN0iyb91Utj0nC1cP3IGjepUZ+LgXpVihKlzLn7KTRRmtsPMnjSzHsCvgf8D1ksaKald3CN0B2Xq0lxuGj2TFg1qMXFwL5o3qJXskJxzaS6WaxRVJF0s6QXgkfB2PPAu8L84x+cOwnufbWTg2Nm0a1yX5wadztH1aiY7JOdcBoil19OXwDTgUTObGrH8OUlnxycsd7DeWLienzw3j5Na1Ofpm3uUORmLc84drFgSxWlmtqO0FWZ2ZwXH4w7BK/PW8POJC+jW5ihG3dSdI2t6knDOVZxYLmYfJekVSRslbZD0kqS2cY7Lxei5mav42cQFnN6+EWNv6eFJwjlX4WJJFM8S1HVqBbQmKA74bDyDcrEZ89FyfvnyIvoe24RRN3WndnUfP+mcq3ixJIoqZjbazPLC25gY93Nx9NSUbH7/n8+44KSmPHV9N2oeUTXZITnnMlSZP0HD6UgBPpD0C+A5gqqxVxOcVcSVpKrAbGCtmV0a7+OlCzPjkfe/5OH3vuS7pzbnoatO5Yiqnredc/ETra1iMUFiKC7h8ZOIdQb8KV5BRRxvCVCvvA0rCzPjb//7gqemZPODbi352/c7U9XndnbOxVmZicLMWiUykEiSWgKXAH8GfpasOFLNg28HSeK601vzx8tOpoonCedcAqRqm8XDwD1EzNFdkqRBkmZLmp2bm5u4yJJk9da9DJ2SzQ+7teT/Lvck4ZxLnJRLFJIuBTaZ2Zxo25nZMDPLMrOsJk0yfzrPER/mULWK+MUFx+EFfZ1ziZRyiQI4E7hM0gqCC+jnSBqX3JCSa+uePJ6fvZoru7agqZflcM4lWLReT52j7WhmCys+HDCz+4D7whj6Ar8ws+vicax08fQnK9ifX8Sgs9snOxTnXCUUrdfT4+G/NQhKjS8m6AF1EjAL6BXf0BzAvrxCxn68gvNOaErHo33iIedc4pXZ9GRmvc2sN5ANdDezLmZ2KtCNoNtq3JnZ5Mo+hmLi7NVs25vP7X38bMI5lxyxXKM4wczmFz8wswXAafELyRUrKCxi+Ic5dGtzFFltGyY7HOdcJRVLcaClkp4CxhEMtLsOWBrXqBwAb366gTXb9vHbS09MdijOuUosljOKGwman+4FfgnkhMtcHJkZQ6dk06FJHc47oWmyw3HOVWLlnlGY2T5JjwCvmNmyBMTkgI+WbWHxup08+P3OPrjOOZdUsUyFeimwiGDqUyR1kfRKvAOr7IZOzeboI2twedfmyQ7FOVfJxdL09AegJ7AdILyw3TGeQVV2n67dwYdfbuaWs9pRo5qXD3fOJVcsiSLfzLaXWGbxCMYFhk3NoW6NavTv2TrZoTjnXEyJYomkq4AqktpJehiYHue4Kq3VW/fyxqL1DOjZmno+ralzLgXEkiiGEAyyKwJeAQ4Ad8czqMps5LTlVBHcfGa7ZIfinHNAbL2e9hB0jb03/uFUblv35PHcrFVc0aUFzep78T/nXGooN1FI6kgweVDbyO3N7DvxC6tyeuaTlV78zzmXcmIZmf0iMJJgZHZhfMOpvPblFTL2kxWcd8LRdGrqxf+cc6kjlkRRZGaPxj2SCJJaAU8DzQiujQwzs0cSGUOivThnNVv35DG4T4dkh+Kcc98Qy8Xs18JpR5tIqld8i3NcBcDPzewE4HTgLkkZW/AoKP63nNNaNyCrzVHJDsc5574hljOKgeG/v4lYZkDcOvmb2XpgfXh/l6QlQAvgs3gdM5n+t3gDq7bu5VeXnODTnDrnUk4svZ5aJSKQskhqSzBx0owSywcBgwBat07fgWlB8b8c2jeuw/le/M85l4KiTYXax8ymSLqstPVm9nr8wvoqhrrAS8DdZrazxPGHAcMAsrKy0nak+CfZW1i0dgcPfO8UL/7nnEtJ0c4ozgemAD8sZZ0BcU0Uko4gSBLjzezleB4rmZ6amkOTI2twRdcWyQ7FOedKVWaiMLNfh/9en7hwAgoa6kcCS8zsoUQfP1E+W7eTqUtzuefC46h5hBf/c86lplguZiPpAuAk4Kvhwmb2l3gFBZwJXA8sklQ8Dev9ZvZmHI+ZcMOmZlOnelUG9GyT7FCcc65MsYzMfgJoAJwNjAa+T5yLAprZNCCjG+zXbNvLfxau55Yz21K/lhf/c86lrljGUZxlZv2BLWb2G4K5KVrGN6zMN3LacgTccpYX/3POpbZYEsW+8N/9kpoB+wnqPrlDtG1PHs/NXM3lXVpwTP1ayQ7HOeeiiuUaxVuSGgD/AOYT1Ht6Oq5RZbhx01eyL7/Qi/8559JCLIn/CsvxAAAXLklEQVTiT2ZWALwg6b9ALWBnOfu4MuzPL2TMxys45/ijOa6ZF/9zzqW+WJqeZhbfMbN9ZrY1cpk7OC/OWcOWPXkM9rMJ51yaiDYy+2jgGKCWpFP4uhdSPaB2AmLLOIVFxvAPc+jSqgE92jVMdjjOOReTaE1PlwC3EPRweiJi+S6+WSDQxejtxRtYuWUv9110vBf/c86ljWgjs0cDoyVdZWYTExhTRgqK/2XTrnEdzj+xWbLDcc65mMVyjeJdSQ9JmilphqR/SvJJEw7S9JytLFizg9t6t6eqF/9zzqWRWBLFcwTNTQOA6wh6PD0fz6Ay0dCp2TSuW53vnebF/5xz6SWW7rGNzex3EY//IGlOvALKREvW72TyF7n8vwu8+J9zLv3EckYxRdIPih9I+h7wVvxCyjzDpuZQu3pVrvPif865NFRmopC0TdJW4GZgoqQ8SQeAF4E74x2YpAslfSFpmaRfxvt48bJm215eX7COa3u0pn5tL/7nnEs/0ZqeGicsihIkVQUeJ5g8aQ0wS9LrZpZ2c2aPmrbCi/8559JatO6xhcX3JV1MUGYcYLKZ/S/OcfUAlplZTnj854DLgbRKFNv35vHcrFVcdmpzWjTw4n/OufRU7jUKSX8G7gFywts9kv4U57haAKsjHq8Jl0XGNUjSbEmzc3Nz4xzOoRk3fSV78woZ1MfLdTjn0lcsvZ6+C3QtPsOQNAqYC/w6jnGVNtDAvvHAbBgwDCArK8tK2T6piov/9T2uCcc3q5fscJxz7pDF0usJgvpOxRJR8nQN0CricUtgXQKOW2FemruGzbvzGHx2h2SH4pxzhyWWM4oHgbmS3if4pd8X+G08gwJmAZ0ktQPWAtcA/eN8zApTWGQMn5rDqS3rc3p7L/7nnEtv5SYKMxsnaRLBFKgCfmtma+MZlJkVSBoCvA1UBUaZ2eJ4HrMivbN4Ayu27OWJAad58T/nXNqL5YyCMDG8HOdYSh7zTeDNRB6zIpgZT03Jpk2j2lxwkhf/c86lv1ivUbgYzVjuxf+cc5nFE0UFGzolm0Z1qvODbi2THYpzzlWImBKFpNMl3RDebySpdXzDSk+fb9jJpC9yuemMtl78zzmXMcq9RiHp18CZQAfgaaAmMAE4K76hpZ/i4n/X9/Lif865zBHLGcUPgIuBPfDVhW0fQVbCuu37eH3+Oq7p3poGtasnOxznnKswsSSKA2ZmhCOjJdWOb0jpadS05Rhwa28v/uecyyyxJIqXJT0O1Jd0M/AOMCq+YaWXHXvzeXamF/9zzmWmWAbc/U3SRUAecCrwZzPziYsijJuxkj15hQw624v/OecyT9REEc4L8aaZXYDPaleq/fmFjP5oBX2ObcIJx/ilG+dc5ona9BRWjM2T5N+AZXhl3lo27z7AYC8l7pzLULGU8NgNLJD0DmHPJwAz+1ncokoTxcX/OresT6/2jZIdjnPOxUUsieK98OZKePezjeRs3sPj/b34n3Muc8VyMXukpGpAx3DRMjMriFdAkv5OMFlSHpAN3Gxm2+N1vENVXPyvdcPaXHiyF/9zzmWuWKZC7Q0sA0YSdItdKunMOMb0LnCymXUGlgL3xfFYh2zWim3MX72d28724n/OucwWS9PTv4CLzewzAEknAM8AWfEIyMzeiXg4nWBkeMopLv73Qy/+55zLcLEMuKtenCQAzGwJkKgaFbdQRrdcSYMkzZY0Ozc3N0HhBJZu3MX7n2/iRi/+55yrBGI5o5graSjBWQTAAGDe4RxU0ntAaQ37vzKz18JtfgUUAONLew4zGwYMA8jKyrLDiedgDZuaQ60jqnL96V78zzmX+WJJFLcDPwbuIZgKdSrw6OEc1MzOi7Ze0o3ApcC5YZ2plLF+xz5em7+WAT3bcFQdL/7nnMt8MU2FCvzDzB4EkFSFODY9SboQuBfoY2Z743WcQzX6oxUUGdx6lhf/c85VDrFco5gE1Il4XAf4ID7hAPAYcCTwrqT5kp6K47EOyo59+UyYsYpLOx9Dq4ZeRNc5VznEckZRy8x2FT8ws13xLDVuZh3L3yo5JsxYxe4DBV78zzlXqcRyRrFX0qnFDyR1AfbHL6TUdKCgkFEfLad3p8ac1Lx+ssNxzrmEieWM4qfAK5JWho9bA9fGL6TU9MrcteTuOsDDV3dJdijOOZdQsZTwmBEOsjuBoNfTYjPLi3tkKaSoyBg2NYeTW9TjjA5e/M85V7nEUsLjewSD7uYDFwDjwuanSuPdJUHxv8Fnd/Dif865SieWaxS/Dy9gn0FQrO95IGV6IsVbcfG/Vg1rcZEX/3POVUKxJIrC8N9LgSfM7CWgRvxCSi2zV25j3qrt3Na7PdWqxvJ2OedcZonlYvZ6SY8DFwJZkqoTW4LJCEOnZHNU7SP4YbdWyQ7FOeeSIpYv/KuAKcAlZrYNaAz8Mq5RpYgvN+7ivSVB8b9a1b34n3Oucoql19NuYGLE43XAungGlSqGTc2h5hFVuKFX22SH4pxzSVNpmpAO1oYd+3l1/lquzmpFQy/+55yrxDxRlGH0R8spLDIG9vZyHc65ys0TRSl27s9n/IxVXNK5uRf/c85VemUmCknbJG0t5bZN0tZ4BybpF5JMUuN4H6uk4uJ/g734n3PORb2YnfAv6GKSWgHnA6sSfewDBYWMmracszo25uQWXvzPOefKPKMws8LIG1AfaBpxi6d/Ecyol/DZ7V6bt45Nuw4wuI+fTTjnHMRW6+kSSUuBNcCM8N+4TVwk6TJgrZktKGe7QZJmS5qdm5tbIccuKjKGTs3mxGPqcVbHpJ1QOedcSollZPafgTOBd8ysq6Tzge8fzkElvQeUVjjpV8D9wHfKew4zGwYMA8jKyqqQM4/3P99Edu4eHrmmixf/c865UCyJosDMciVVkSQze1fSnw/noGZ2XmnLJZ0CtAMWhF/ULYG5knqY2YbDOWYshk7JpkWDWlxyyjHxPpRzzqWNWBLFDkl1gGnA05I2AUXxCMbMFgFHFz+WtALIMrPN8ThepNkrtjJ75TZ+/90Tvfifc85FiOUb8QqCqU/vBiYDawkqyWaUoVNzaFD7CK7q7sX/nHMuUiyJ4r6w51O+mY00s4eAn8U7MAAza5uIs4llm3bz7mcbuaFXW2pXj+UkyznnKo9YEsWFpSy7pKIDSabhU3OoUa0KN/Zqk+xQnHMu5ZT581nSYOB24FhJcyNWHQnMjndgibJx535embeWq7u3olHdSjMfk3POxSxaO8tE4H3gr3xz/oldZrYprlEl0OiPVlBQVMTA3u2SHYpzzqWkMhNFOEnRNuCHkk4GzgpXfQhkRKLYtT+f8dNXctEpx9CmUZ1kh+OccykplpHZdxGcXbQObxMl3RnvwBLhzUXr2eXF/5xzLqpYuvgMBnqEM90h6S/Ax8AT8QwsEa7KasVxzerRuWWDZIfinHMpK5ZeTwLyIx7nh8vSniS6tPIk4Zxz0UTr9VTNzAqAZ4Dpkl4KV10JjE1EcM4555IvWtPTTOA0M3tQ0iSgN8GZxO1mNish0TnnnEu6aIniq+alMDF4cnDOuUooWqJoIqnMUh1hKQ/nnHMZLlqiqArUJUMuXDvnnDs00RLFejP7Y8IiiSDpR8AQoAB4w8zuSUYczjnnYrxGkUiS+gGXA53N7ICko8vbxznnXPxEG0dxbsKi+KY7gAfM7ABAJtWVcs65dFRmojCzrYkMJMKxQG9JMyRNkdS9tI0kDZI0W9Ls3NzcBIfonHOVR1Jm6ZH0HtCslFW/IojpKOB0oDtBban2ZmaRG5rZMGAYQFZWlpV8IueccxUjKYnCzM4ra52kO4CXw8QwU1IR0Bjw0wbnnEuCWGo9JdqrwDkAko4FqgNxnw7VOedc6VJxguhRwChJnwJ5wI0lm52cc84lTsolCjPLA65LdhzOOecCqdj05JxzLoV4onDOOReVJwrnnHNReaJwzjkXlScK55xzUXmicM45F5UnCuecc1F5onDOOReVJwrnnHNReaJwzjkXlScK55xzUXmicM45F1XKJQpJXSRNlzQ/nMGuR7Jjcs65yizlEgXwIPAHM+sC/DZ87JxzLklSMVEYUC+8Xx9Yl8RYnHOu0ku5+SiAu4G3Jf2DIJGdUdpGkgYBgwBat26duOicc66SSUqikPQe0KyUVb8CzgV+amYvSboKGAl8a45tMxsGDAPIysryGfCccy5OkpIozOxbX/zFJD0N/CR8+AIwIiFBOeecK1UqXqNYB/QJ758DfJnEWJxzrtJLxWsUtwGPSKoG7Ce8DuGccy45Ui5RmNk0oFuy43DOORdIxaYn55xzKcQThXPOuag8UTjnnIvKE4VzzrmoPFE455yLyhOFc865qDxROOeci8oThXPOuag8UTjnnIvKE4VzzrmoPFE455yLyhOFc865qJKSKCT9UNJiSUWSskqsu0/SMklfSLogGfE555z7WrKqx34KfA8YGrlQ0onANcBJQHPgPUnHmllh4kN0zjkHSTqjMLMlZvZFKasuB54zswNmthxYBvRIbHTOOecipdp8FC2A6RGP14TLvkXSIL6e1Gi3pNISTywaA5sPcd9kSKd40ylWSK940ylWSK940ylWOLx428SyUdwShaT3gGalrPqVmb1W1m6lLLPSNjSzYcCwQwzv6wNKs80sq/wtU0M6xZtOsUJ6xZtOsUJ6xZtOsUJi4o1bojCz8w5htzVAq4jHLQnm0HbOOZckqdY99nXgGkk1JLUDOgEzkxyTc85VasnqHnulpDVAL+ANSW8DmNliYCLwGfA/4K4E9Hg67OarBEuneNMpVkiveNMpVkiveNMpVkhAvDIr9RKAc845B6Re05NzzrkU44nCOedcVJU6UUi6MCwVskzSL5MdTzSSRknaJOnTZMdSHkmtJE2StCQs1fKTZMdUFkk1Jc2UtCCM9Q/JjikWkqpKmifpv8mOJRpJKyQtkjRf0uxkx1MeSQ0kvSjp8/Dz2yvZMZVG0nHhe1p82ynp7rgdr7Jeo5BUFVgKnE/QLXcWcK2ZfZbUwMog6WxgN/C0mZ2c7HiikXQMcIyZzZV0JDAHuCIV31tJAuqY2W5JRwDTgJ+Y2fRydk0qST8DsoB6ZnZpsuMpi6QVQJaZpcUANkljgQ/NbISk6kBtM9ue7LiiCb/L1gI9zWxlPI5Rmc8oegDLzCzHzPKA5whKiKQkM5sKbE12HLEws/VmNje8vwtYQhkj7JPNArvDh0eEt5T+9SSpJXAJMCLZsWQSSfWAs4GRAGaWl+pJInQukB2vJAGVO1G0AFZHPC6zXIg7dJLaAl2BGcmNpGxhM858YBPwrpmlbKyhh4F7gKJkBxIDA96RNCcsu5PK2gO5wOiwWW+EpDrJDioG1wDPxvMAlTlRxFwuxB0aSXWBl4C7zWxnsuMpi5kVmlkXgkoAPSSlbNOepEuBTWY2J9mxxOhMMzsNuAi4K2xCTVXVgNOAJ82sK7AHSPVrl9WBy4AX4nmcypwovFxIHIXt/S8B483s5WTHE4uwmWEycGGSQ4nmTOCysO3/OeAcSeOSG1LZzGxd+O8m4BVSuxr0GmBNxBnliwSJI5VdBMw1s43xPEhlThSzgE6S2oVZ+RqCEiLuMIUXiEcCS8zsoWTHE42kJpIahPdrAecBnyc3qrKZ2X1m1tLM2hJ8Zj8ws+uSHFapJNUJOzMQNuF8h2AumpRkZhuA1ZKOCxedS1AlIpVdS5ybnSD1yownjJkVSBoCvA1UBUaFJURSkqRngb5A47D8ye/MbGRyoyrTmcD1wKKw7R/gfjN7M4kxleUYYGzYc6QKMNHMUrrLaRppCrwS/G6gGjDBzP6X3JDK9SNgfPjjMQe4OcnxlElSbYJem4PjfqzK2j3WOedcbCpz05NzzrkYeKJwzjkXlScK55xzUXmicM45F5UnCuecc1F5onCHRVJTSRMk5YRlGj6RdGWy4yqLpMmS4jYRvaS+ZVV0ldRV0ojw/mUHU7E4rGp6ZyzHSQWS2kardCypuqSpkiptF/104onCHbJwYN2rwFQza29m3QgGgbVMbmQp637gUQAze93MHjiIfRsAd5a7VZoIC3G+D1yd7Fhc+TxRuMNxDpBnZk8VLzCzlWb2KHz1q/JDSXPD2xnh8r6SpkiaKGmppAckDQjnhVgkqUO4XRNJL0maFd7ODJf3iajDP6949G+x8LifSxoraWE4v0DtksFLelLSbEXMQyHpXEmvRGxzvqSXw/vfCc+Y5kp6IaxlVTyvyeeSpgHfK+2NCmPsbGYLwsc3SXosvD9G0r8lfRyemf2glKd4AOgQvua/h8vq6uu5E8aHibv4NcwL38tRkmqEy1dIahzez5I0uaz3U1JdSe+Hr3WRpMsj3tslkoaH79s74Yh2JHVTMK/HJ8BdEa/9pPD/dn74/9EpXPUqMKC098ulGDPzm98O6Qb8GPhXlPW1gZrh/U7A7PB+X2A7wajoGgS19P8QrvsJ8HB4fwJwVni/NUFJEID/EBSbA6gLVCtx3LYEBR6LtxkF/CK8P5lgfgSAhuG/VcPlnQmKRX4ONImI4btAY2AqwdwVAPcCvwVqElQh7hTuOxH4bynvRT/gpYjHNwGPhffHEBR1qwKcSFD+vuT+bYFPIx73BXYQnL1VAT4BzoqI59hwu6cJijICrAAah/ezgMllvZ/hrV64rDGwLHx9bYECoEu4biJwXXh/IdAnvP/34ngJzqIGhPerA7Ui3vfcZH+O/Vb+zc8oXIWR9Hj4i3JWuOgIYLikRQRfhCdGbD7LgnkrDgDZwDvh8kUEX0YQ1F16TEEZkNeBeuEv84+AhyT9GGhgZgWlhLPazD4K748j+BIt6SpJc4F5wEnAiRZ8gz0DXKegBlQv4C3g9DD+j8J4bgTaAMcDy83sy3Dfsgr0HUNQwrosr5pZkQWTOzWNsl2kmWa2xsyKgPkE79txYTxLw23GEsyxEE1p76eAv0haCLxHUIK/OK7lZlZcmmUO0FZS/XDfKeHyZyKe/xPgfkn3Am3MbB8EVXuBvJJnhC71+IUkdzgWA98vfmBmd4VNG8VTXv4U2AicSvCrd3/Evgci7hdFPC7i689lFaBX8RdLhAckvQFcDEyXdJ6ZlSzkV7I2zTceS2oH/ALobmbbJI0h+DUOMJrgV/Z+4AUL6oKJYK6Ka0s8T5dSjlWafRHPX5rI96O0Evjl7VNI8L5F27eAr5ubv4rFzL71fhIkxiZANzPLV1CttnifksetFR631PfBzCZImkEw2dLbkgaa2Qfh6hp883PhUpCfUbjD8QFQU9IdEcsirwXUB9aHv3ivJ2hqOBjvAEOKH4RfykjqYGaLzOxvBEnp+FL2ba2v5zu+lmCK00j1COYb2CGpKUG5ZuCr0tjrgF8TNAsBTAfOlNQxjKG2pGMJmqnaFV9XCY9VmiVAx3Jfcdl2AbH88v6c4Bd+8bGuB4p/5a8AuoX3v0rwZbyf9QnmvciX1I/g7KlMFpRo3yGp+Mztq2sPktoDOWb2b4Izw87h8kYETU/5Mbwul0SeKNwhC5targD6SFouaSZBU8e94SZPADdKmg4cS/DFfDB+DGSFF0A/A24Pl98t6VNJCwh+qb9Vyr5LwmMvBBoCT5aIfQFBk9NigmsYH5XYfzxB89Vn4fa5BNcVng2fczpwvJntBwYBb4QXs0udjjI846l/qM0sZraFoNnr04iL2aVtt5+g4ukLYZNfEVDc2eAPwCOSPiQ4EyhW2vs5nuC9n03wpR9L6fWbgcfDi9mRZ4FXA5+GTXbHE1w3geC6TSpWFHYlePVYl3EUTL/6XzM75Jnqwh5J86wCS7lL+imwy8x8rmsg7E12n5l9kexYXHR+RuFcCZLmEDSPVPTMcU/yzfb9SkvBfA+vepJID35G4ZxzLio/o3DOOReVJwrnnHNReaJwzjkXlScK55xzUXmicM45F9X/B3qt/LC6i/l0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24dbc5d0588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Produced graph in 84.9233078956604 seconds\n"
     ]
    }
   ],
   "source": [
    "### code should run if cell containing main code has been run first\n",
    "### if error comes up run the massive chunk of code first, this will just initialise everything and won't try to print anything\n",
    "\n",
    "import time\n",
    "\n",
    "games = 40000\n",
    "n = 5000\n",
    "agents = 1\n",
    "show_random = False\n",
    "graph_for_wins = False\n",
    "\n",
    "start = time.time()\n",
    "game(games, n, agents, show_random, graph_for_wins)\n",
    "print(\"Produced graph in\",(time.time() - start),\"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2A\n",
    " \n",
    "Using the minimax policy you computed, answer the following question: The first player (Player 1) drops his/her first disk into column 2 (counting from the left). Consider the resulting state, shown in the following code cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = connect.Connect(verbose=False)\n",
    "env.reset(first_player='o')\n",
    "env.act(action=1)\n",
    "print(env.grid[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the Minimax Value of this state for Player 2? That is, assuming an optimal opponent, does the Minimax Policy expect to win the game (value = 1), lose the game (value = –1), or end the game in a draw (value = 0)? Please state your answer as a number.    \n",
    "\n",
    "* The code cell below should compute this value and assign it to a variable called `state_value`.\n",
    "* Count the number of branches of the game tree that were examined and assign this number to a variable called `num_branches`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7b00948d6cc98e71a2f9467263067bc0",
     "grade": false,
     "grade_id": "cell-9e3a2a1bebc09565",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### Write all your code for Part 2A in or above this cell.\n",
    "\n",
    "# state_value = ...\n",
    "# num_branches = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0d9e96fd7c7ed9d9081eb564773f199c",
     "grade": true,
     "grade_id": "cell-c31d5222d21dd1d5",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograded test cell. Do not delete or change, otherwise you will get \n",
    "# no marks for this part of the assignment. Please make sure that this cell has \n",
    "# access to the variables state_value and num_branches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2B\n",
    "Plot a learning curve similar to the one in Part 1, comparing your Q-learning algorithm, random play, and Minimax play. Assume as before that the opponent always plays first and uses a random policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f6ed321958b16409ec1935b0fef7aa0d",
     "grade": true,
     "grade_id": "cell-a1d1652414bc7967",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### Write all your code for Exercise 2 (B) in or above this cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "614920eb9ba438cf5626dd59edc30252",
     "grade": true,
     "grade_id": "cell-1ea89dfffb81a041",
     "locked": false,
     "points": 10,
     "schema_version": 1,
     "solution": true
    }
   },
   "source": [
    "Explain your findings in 3 or fewer sentences. Which policy is better? Why?\n",
    "\n",
    "YOUR ANSWER HERE.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "\n",
    "Using your algorithm, compute the value of the game for your player (recall: your player goes second against a random opponent). The code cell below should compute this value and assign it to a variable called `optimal_policy_value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d7e4093f906a825959634f4ee6e6845f",
     "grade": false,
     "grade_id": "cell-a28f4f38b5cc6619",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### Write all your code for Part 3 in or above this cell.\n",
    "\n",
    "# optimal_policy_value = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b3a945d86dc159320c257d69389cf765",
     "grade": true,
     "grade_id": "cell-1e0341a7a580c299",
     "locked": true,
     "points": 10,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is an autograded test cell. Do not delete or change, otherwise you will get \n",
    "# no marks for this part of the assignment. Please make sure that this cell has \n",
    "# access to the variable optimal_policy_value."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
